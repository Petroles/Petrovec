{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geração de corpora consolidados, com nível de acesso público"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os corpora são compostos por diversas fontes de dados, com formatos heterogeneos dentro do domínio. \n",
    "Este scripts, portanto, consolidam conjuntos de corpora em um arquivo único, para geração de um modelo.\n",
    "Para cada arquivo consolidado (contendo um conjunto de corpora), será posteriormente gerado um modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "import shutil\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p: ', level=logging.WARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../_Util.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pastaArquivosCorpora = join('..', '..', 'resources', 'corpora', '3- Textos Preprocessados')\n",
    "pastaArquivosCorporaSaida = join('..', '..', 'resources', 'Corpora', '3- Textos Preprocessados', '_Consolidados')\n",
    "\n",
    "# Elementos Parametrizaveis\n",
    "imprimeEstatisticas = True\n",
    "imprimeEstatisticasVocabulario = True\n",
    "exportaArquivoPalavrasRaras = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessaArquivoConsolidado(nomeCorpusSaida, arquivosDeCorpusEntrada):\n",
    "    momentoInicial = datetime.datetime.now()\n",
    "    \n",
    "    arquivoCorpusSaida = join(pastaArquivosCorporaSaida, nomeCorpusSaida)\n",
    "    #arquivoSaidaPalavrasRaras = join(pastaArquivosCorporaSaida, nomeCorpusSaida+\".PalavrasRaras.txt\")\n",
    "\n",
    "    logging.info(\"Unificando arquivos\")\n",
    "    mergeConteudoArquivos(arquivosDeCorpusEntrada, arquivoCorpusSaida)\n",
    "    print(\"\\n* Arquivo de saida gravado em \"+arquivoCorpusSaida)\n",
    "\n",
    "    #logging.info(\"Gravando arquivo de saida\")\n",
    "    #gravaArquivo(arquivoCorpusSaida, textoCorpus)\n",
    "        \n",
    "    textoCorpus = ''\n",
    "        \n",
    "    if imprimeEstatisticas:\n",
    "        print(\"\\n* Estatísticas do corpus final consolidado:\")\n",
    "        textoCorpus = leTextoDeArquivo(arquivoCorpusSaida)\n",
    "        imprime_info_corpus(textoCorpus, geraEstatisticasVocabulario = imprimeEstatisticasVocabulario)\n",
    "    \n",
    "    momentoFinal = datetime.datetime.now()\n",
    "    print(\"\\nPreprocessamento: tempo total decorrido: \", momentoFinal - momentoInicial)  \n",
    "   \n",
    "    return textoCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeConteudoArquivos(listaArquivos, arquivoSaida):\n",
    "    with open(arquivoSaida,'wb') as out_file:\n",
    "        for f in listaArquivos:\n",
    "            with open(join(pastaArquivosCorpora, f),'rb') as input_file:\n",
    "                shutil.copyfileobj(input_file, out_file)\n",
    "                out_file.write(b\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeConteudoArquivos_OLD(arquivos):\n",
    "    conteudoArquivosProcessados = []\n",
    "    for arquivo in arquivos:\n",
    "        pathArquivo = join(pastaArquivosCorpora, arquivo)\n",
    "        texto = leTextoDeArquivo(pathArquivo)\n",
    "        conteudoArquivosProcessados.append(texto)\n",
    "        \n",
    "    textoCorpus = '\\n'.join([arquivo for arquivo in conteudoArquivosProcessados])\n",
    "    \n",
    "    #if imprimeEstatisticas:\n",
    "    #    print(\"* Estatísticas do corpus na entrada:\")\n",
    "    #    imprime_info_corpus(textoCorpus)\n",
    "    \n",
    "    return textoCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geraRelatorioPalavrasRaras(texto, arquivoSaidaPalavrasRaras, frequenciaMinimaPalavra):\n",
    "    word_counts = Counter(texto.split())\n",
    "    palavras_raras = [w for w in word_counts if word_counts[w] <= frequenciaMinimaPalavra]\n",
    "    print('Vocabulário total: ', \"{:,}\".format(len(word_counts.most_common() )))\n",
    "    print('Palavras raras: ', \"{:,}\".format(len(palavras_raras)))\n",
    "    \n",
    "    listaPalavrasRaras = \"\\n\".join([w for w in palavras_raras])\n",
    "    gravaArquivo(arquivoSaidaPalavrasRaras, listaPalavrasRaras)    \n",
    "    return palavras_raras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminaPalavrasRarasESentencasCurtas(texto, arquivoSaidaPalavrasRaras, frequenciaMinimaPalavra = 1):\n",
    "    inicioPalavrasRaras = datetime.datetime.now()\n",
    "    \n",
    "    # apenas grava o arquivo com as palavras raras. As ocorrencias nao estao sendo eliminadas, por restricoes de processamento\n",
    "    # isso sera feito diretamente pela api do gensim, durante o treinamento dos modelos\n",
    "    palavras_raras = geraRelatorioPalavrasRaras(texto, arquivoSaidaPalavrasRaras, frequenciaMinimaPalavra)\n",
    "    \n",
    "    sentences = texto.split('\\n')\n",
    "    \n",
    "    #print('Substituindo palavras raras no texto')    \n",
    "    #sentences = [processSentence(sentence, palavras_raras) for sentence in sentences]\n",
    "    \n",
    "    # unificando caso a sentença contenha ao menos 3 palavras\n",
    "    tamanho_minimo_sentecas = 3\n",
    "    texto = \"\\n\".join( [sentence for sentence in sentences if len(sentence.split()) >= tamanho_minimo_sentecas])  \n",
    "\n",
    "    print(\"Tempo de processamento para eliminar sentenças curtas: \", datetime.datetime.now() - inicioPalavrasRaras)  \n",
    "    \n",
    "    return texto    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processSentence(sentence, palavras_raras):\n",
    "    return \" \".join([palavra for palavra in sentence.split() if palavra not in palavras_raras])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
